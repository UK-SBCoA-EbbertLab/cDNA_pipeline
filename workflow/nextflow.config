// CONFIGURATION FILE

// Pipeline parameter default values, can be modified by user when calling pipeline on command line (e.g. --ont_reads_fq sample_1.fastq) ##
// Alphabetical order, lists the step for the parameter

params.annotation = 'None'			// Steps 2,3 : Input GTF/GFF genomic annotation
params.bai = "None"				// Step 2 : Unfiltered bam index file input if you want to start from the bam filtering step
params.bam = "None"				// Step 2 : Unfiltered bam file input if you want to start from the bam filtering step
params.bambu_rds = "None"			// Step 3 : Directory containing RDS files
params.basecall_compute = "gpu"			// Step 1 : CPU vs GPU basecalling
params.basecall_config = "None"			// Step 1 : Desired basecall configuration
params.basecall_demux = false			// Step 1 : Basecalling demultiplexing
params.basecall_mods = false			// Step 1 : Desired basecaller modifications
params.basecall_path = 'None'			// Step 1 : directory of basecalling data
params.basecall_speed = "hac"			// Step 1 : Desired basecall speed 
params.basecall_trim = "none"			// Step 1 : Type of read trimming during basecalling ("all", "primers", "adapters", "none")
params.cdna_kit = "PCS111"			// Step 2 : cDNA sequencing kit adapters for Pychopper to trim and orient reads
params.contamination_ref = "None"		// Step 2 : Reference file for contamination analysis
params.ctat_lib_dir = "None"			// Step 2 : The location to the ctat_genome_lib_build_dir directory (see https://github.com/TrinityCTAT/CTAT-LR-fusion/wiki#obtaining-and-configuring-the-ctat-genome-lib for more information) 
params.ercc = "None"				// Step 2 : Want to concatenate ERCC GTF to CHM13 GFF? Add the ERCC gtf file here
params.fai = "None"				// Step 3 : Index file for reference genome
params.housekeeping = 'None'			// Step 2 : Input bed file for housekeeping genes (RSEQc)
params.intermediate_qc = "None"			// Step 3 : Intermediate QC reports
params.is_chm13 = false				// Steps 2,3 : Logical, is the reference CHM13?
params.is_discovery = "None"			// Step 3 : Logical, do you want to perform discovery using Bambu? True = Yes
params.is_dRNA = false				// Step 2 : Binary boolean parameter to check if user is performing Direct RNAseq analysis
params.mapq = "0"				// Step 2 : MAPQ filtering threshold for bam files, 0 for no filtering
params.multiqc_config = "None"			// Step 3 : MultiQC configuration file
params.multiqc_input = "None"			// Step 3 : Directory with MultiQC input for making report.
params.new_gene_and_isoform_prefix = "Bambu"	// Step 3 : Prefix for bambu discovered genes and isoforms.
params.NDR = "auto"				// Step 3 : NDR value for Bambu novel discovery filtering - Leave it on Auto for most applications
params.ont_reads_fq = 'None'			// Step 2 : Input fastq reads
params.ont_reads_txt = 'None'			// Step 2 : Input sequencing summary files
params.out_dir = "output_directory/"		// All : Output directory for pipeline results
params.path = 'None'				// Step 2 :  Input unzipped "raw" ONT output files
params.prefix = "None"				// Steps 1,2 : Add prefix to all output files
params.qscore_thresh = "9"			// Steps 1,2 : Quality score threshold
params.quantification_tool = "bambu"		// Steps 2,3 : Which quantification tools should be run? (bambu, isoquant, both)
params.ref = 'None'				// Steps 2,3 : Input reference fasta file
params.rseqc_tin = false			// Step 2 : Perform RSEQC TIN?
params.step = "None"				// All : Which step of the pipeline to perform. 1 = Basecalling, 2 = Pre-processing, 3 = Discovery and quantification
params.track_reads = false			// Step 3 : Logical, Track Bambu read assignments. True = Track. Tracking uses more memory, but allows you to extract reads that align to specific transcripts
params.trim_barcode = "True"			// Step 1 : Trime barcodes (only counts if demultiplexing is enabled)
params.trim_dRNA = false			// Step 2 : Trim dRNA adapters and primers?


process { 

    // Define job scheduler parameters for jobs that require little memory computation/memory ##

    withLabel: tiny {
        executor='slurm'
        clusterOptions='--partition normal --time 00:15:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 4 --mem-per-cpu 4G'
    }



    withLabel: small {
        executor='slurm'
        clusterOptions='--partition normal --time 1:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 8 --mem-per-cpu 4G'
    }



    // Define job scheduler parameters for jobs that require medium computation/memory ##

    withLabel: medium_small {
        
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 12 --mem-per-cpu 4G'
        }


    withLabel: medium {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 16 --mem-per-cpu 4G'
        }

    withLabel: medium_large {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 20 --mem-per-cpu 4G'
    }

    // Define job scheduler parameters for jobs that require lots of computation/memory ##

    withLabel: large {
        executor='slurm'
        clusterOptions='--partition normal --time 40:00:00 --account coa_mteb223_uksr --nodes 1 --ntasks 1 --cpus-per-task 50 --mem-per-cpu 4G' 
    }
    
    withLabel: rseqc {
        executor='slurm'
        clusterOptions='--partition normal --time 7-00:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 50 --mem-per-cpu 4G' 
    }
 

    withLabel: bambu_prep_job {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 12 --mem-per-cpu 16G'
        }

    withLabel: huge {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account coa_mteb223_uksr --nodes 1 --ntasks 1 --cpus-per-task 12 --mem-per-cpu 40G'
    }

    withLabel: huge_long {
        executor='slurm'
        clusterOptions='--partition normal --time 7-00:00:00 --account cca_mteb223_uksr --nodes 1 --ntasks 1 --cpus-per-task 12 --mem-per-cpu 40G'
    }


    withLabel: contamination {
        executor='slurm'
        clusterOptions='--partition normal --time 23:00:00 --account coa_jbmi230_uksr --nodes 1 --ntasks 1 --cpus-per-task 50 --mem-per-cpu 10G'
    }

    // Define local execution

    withLabel: local {
        executor='local'
    }

    withLabel: gpu {
        
        executor='slurm'
        clusterOptions='--partition P4V12_SKY32M192_L --time 7-00:00:00 --account gol_mteb223_uksr --gres=gpu:1 --mem 16G'
        containerOptions = '--nv'
    }
    
    // Define cluster options for BAMBU_DUSCOVERY and BAMBU_QUANT
    withLabel: bambu_discovery {
        executor='slurm'
        clusterOptions='--partition normal --time 14-00:00:00 --account cca_mteb223_uksr --nodes 1 --mem-per-cpu 50G --ntasks 1 --cpus-per-task 10'
    }


    // Define the singularity containers for each process, will pull containers from the cloud

    // Nanopore
    withName: "(TRIM_dRNA|MAKE_FAI|FIX_SEQUENCING_SUMMARY_NAME|UNZIP_AND_CONCATENATE|MAP_CONTAMINATION_dRNA|MAP_CONTAMINATION_cDNA|GFFCOMPARE|MAKE_INDEX_cDNA|MAKE_INDEX_dRNA|MINIMAP2_cDNA|MINIMAP2_dRNA|MINIMAP2_QC|FILTER_BAM|PYCHOPPER|MAKE_TRANSCRIPTOME|MAKE_INDEX_cDNA_CONTAMINATION_CHM13|MAKE_INDEX_dRNA_CONTAMINATION_CHM13)" {
        container = "library://ebbertlab/nanopore_cdna/nanopore:sha256.df8b78d8644d58861e7ea5c82c218b76845559c0d71fdb45e58d271a349fd045" 
   }

    // Quality Control
    withName: "(MERGE_QC_REPORT|MAKE_QC_REPORT_TRIM|MAKE_QC_REPORT_NO_TRIM|MULTIQC_GRCh38|MULTIQC_CHM13|RSEQC_TIN|RSEQC_GENE_BODY_COVERAGE|RSEQC_BAM_STAT|RSEQC_READ_GC|CONVERT_GTF_TO_BED12|RSEQC_JUNCTION_ANNOTATION|RSEQC_JUNCTION_SATURATION|RSEQC_READ_DISTRIBUTION|PYCOQC|PYCOQC_dRNA|DECOMPRESS|TRIM_GALORE|CHM13_GTF_ERCC|CHM13_GTF)" {
        container = "library://ebbertlab/nanopore_cdna/quality_control:sha256.45fd0d3aa770abea5c195a734c139250f73af2763f8ae10e03c4751143844bb4"
    }

    // Basecalling
    withName: "(FAST5_to_POD5|BASECALL_CPU|BASECALL_CPU_DEMUX|BASECALL_GPU|BASECALL_GPU_DEMUX)" {
        container = "library://ebbertlab/nanopore_cdna/dorado:sha256.4cec2a7db51075e2480ac8b75a7b12c4e77727aa779ccb07925d77ed31283cbd"
    }

    // Bambu
    withName: "(BAMBU_PREP|BAMBU_DISCOVERY|BAMBU_QUANT)" {
        container = "library://ebbertlab/nanopore_cdna/bambu:sha256.44e2b6d7282a488b95b132198b7c4ca659c9e8d6a83797493e746aa3a87ecfea"
    }

    // IsoQuant
    withName: "(ISOQUANT)" {
        container = "library://ebbertlab/big_experiment/isoquant:sha256.27a9e4e797c94b7a837ba70e021dee47e3bb3dfa015ad21e2d19696d75f2628c"
    }

    // CTAT-LR-fusion
    withName: "(CTAT_LR_FUSION)" {
        container = "library://ebbertlab/big_experiment/ctat_lr_fusion:sha256.bc8b34472e2ff501a80383a93ce289798cf697c6f8dc28f37ae352a993f28d31"
    }
}


// Define executor type and maximum queue size for jobs at once ##

executor {

    name="slurm"
    queueSize = 50
}

// Point to singularity image with the tools necessary to run the pipeline

singularity {
    
    enabled = true
}

